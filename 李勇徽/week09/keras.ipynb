{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73aa4225-3d6d-4f7d-8f86-77269ac782df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6cb95e1-1427-488d-8b1f-829415af513f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'macOS-14.2.1-arm64-arm-64bit'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "platform.platform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01c233fa-e26d-4337-8ca7-c69eb2e54615",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv ~/venv-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2a5cdf-9e12-49d6-b5ab-41535f8fb89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/venv-metal/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7fbb97a-8149-42b0-8c28-fcf7a8ad6b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (24.0)\n",
      "Collecting pip\n",
      "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "Successfully installed pip-24.3.1\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c266553-1324-4bb2-b7e1-f93ae9e603f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.68.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl (239.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.68.0-cp312-cp312-macosx_10_9_universal2.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl (405 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp312-cp312-macosx_11_0_arm64.whl (322 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.0 keras-3.6.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12e4e80-2ce5-4f4f-b71b-dc48b1eb5f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75ba5c-69d5-440c-8949-c99cd2fc3f32",
   "metadata": {},
   "source": [
    "# 1.将训练数据和检测数据加载到内存中(第一次运行需要下载数据，会比较慢):\n",
    "train_images是用于训练系统的手写数字图片;\n",
    "train_labels是用于标注图片的信息;\n",
    "test_images是用于检测系统训练效果的图片；\n",
    "test_labels是test_images图片对应的数字标签。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b479b766-42f6-420d-8c69-162d70bdb9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape =  (60000, 28, 28)\n",
      "tran_labels =  [5 0 4 ... 5 6 8]\n",
      "test_images.shape =  (10000, 28, 28)\n",
      "test_labels [7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print('train_images.shape = ',train_images.shape)\n",
    "print('tran_labels = ', train_labels)\n",
    "print('test_images.shape = ', test_images.shape)\n",
    "print('test_labels', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c113af09-2931-4298-bdd7-d2d96550b289",
   "metadata": {},
   "source": [
    "# 2. 打印用于测试的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4172858-aee9-4ca7-b226-cc7f107ce6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = test_images[0]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f43df-1d61-438d-b5d3-068834e676f3",
   "metadata": {},
   "source": [
    "# 3.使用tensorflow.Keras搭建一个有效识别图案的神经网络，\n",
    "1. layers:表示神经网络中的一个数据处理层。(dense:全连接层)\n",
    "2. models.Sequential():表示把每一个数据处理层串联起来.\n",
    "3. layers.Dense(…):构造一个数据处理层。\n",
    "4. input_shape(28*28,):表示当前处理层接收的数据格式必须是长和宽都是28的二维数组，后面的“,“表示数组里面的每一个元素到底包含多少个数字都没有关系.\n",
    "5. network.compile() 的作用：compile() 是 Keras 中用于配置模型的函数。在训练模型之前，需要通过它指定：\n",
    "    - 优化器（如何更新权重）\n",
    "    - 损失函数（衡量模型输出与目标值的差距）\n",
    "    - 评估指标（用于评估模型性能）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85909a1b-cb00-406d-9b9a-9ac602577e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c84f96-a57d-4911-8b8a-81cfb7b235f4",
   "metadata": {},
   "source": [
    "# 4. 在把数据输入到网络模型之前，把数据做归一化处理\n",
    "1. reshape(60000, 28\\*28）:train_images数组原来含有60000个元素，每个元素是一个28行，28列的二维数组，\n",
    "现在把每个二维数组转变为一个含有28\\*28个元素的一维数组.\n",
    "2. 由于数字图案是一个灰度图，图片中每个像素点值的大小范围在0到255之间.\n",
    "3. train_images.astype(“float32”)/255 把每个像素点的值从范围0-255转变为范围在0-1之间的浮点值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4239185-1c00-43e0-a9c6-0775a25f6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28*28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab64320-dadb-45fd-b68d-026a422095d5",
   "metadata": {},
   "source": [
    "把图片对应的标记也做一个更改：\n",
    "\n",
    "目前所有图片的数字图案对应的是0到9。\n",
    "\n",
    "例如test_images\\[0]对应的是数字7的手写图案，那么其对应的标记test_labels\\[0]的值就是7。\n",
    "\n",
    "我们需要把数值7变成一个含有10个元素的数组，然后在第8个元素设置为1，其他元素设置为0。\n",
    "\n",
    "例如test_lables\\[0] 的值由7转变为数组\\[0,0,0,0,0,0,0,1,0,0] ---one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bd444dd-b75a-42fa-8e91-efe1fc9297bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before change: 7\n",
      "after change:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "print(\"before change:\" ,test_labels[0])\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "print(\"after change: \", test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe892de-5925-478e-ad32-649ffe375428",
   "metadata": {},
   "source": [
    "# 5. 训练\n",
    "- train_images：用于训练的手写数字图片；\n",
    "- train_labels：对应的是图片的标记；\n",
    "- batch_size：每次网络从输入的图片数组中随机选取128个作为一组进行计算。\n",
    "- epochs: 每次计算的循环是五次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd0dc0e9-d618-4c67-aa62-93527a5d2dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8735 - loss: 0.4407\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 0.1138\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9783 - loss: 0.0723\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9852 - loss: 0.0503\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9892 - loss: 0.0370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2844ff4a0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b78d5c-b05a-42ae-b028-bb2ba6cc6208",
   "metadata": {},
   "source": [
    "# 6. 测试数据，检验学习后的图片识别效果\n",
    "识别效果与硬件（CPU/GPU）有关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42394df7-4e2d-47b8-87d6-c3be380126d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9778 - loss: 0.0730\n",
      "0.060919299721717834\n",
      "test_acc 0.9817000031471252\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels, verbose=1)\n",
    "print(test_loss) \n",
    "print('test_acc', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d02b3d-22fe-4ffd-bfae-b01d2db1d6a2",
   "metadata": {},
   "source": [
    "# 7. 输入一张手写数字图片，查看识别效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3da7bea0-021c-48e3-8b0e-93a58b78f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2  # OpenCV 用于图像处理\n",
    "image_path = \"picture.png\"  # 替换为实际图片路径\n",
    "img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)  # 读取原始图像，保持4通道\n",
    "rgb = img[:, :, :3]\n",
    "gray_img = 0.2989 * rgb[:, :, 0] + 0.5870 * rgb[:, :, 1] + 0.1140 * rgb[:, :, 2]\n",
    "resized_img = cv2.resize(gray_img, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "flattened_array = resized_img.flatten()\n",
    "final_array = flattened_array.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7310755c-e11c-48f0-99b2-109156a347d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n"
     ]
    }
   ],
   "source": [
    "res = network.predict(final_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0924117a-08b5-4b2d-9262-5f5b3b2de034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number for the picture is :  3\n"
     ]
    }
   ],
   "source": [
    "for i in range(res[0].shape[0]):\n",
    "    if (res[0][i] == 1):\n",
    "        print(\"the number for the picture is : \", i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f7730c0e-6f6e-4541-aa48-2c0ccd242651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# test_image = img.reshape((1000000, 28*28))\n",
    "rgb = img[:, :, :3]\n",
    "# 使用灰度公式转换\n",
    "gray_img = 0.2989 * rgb[:, :, 0] + 0.5870 * rgb[:, :, 1] + 0.1140 * rgb[:, :, 2] * 255\n",
    "# 2. 调整尺寸为 28x28\n",
    "resized_img = cv2.resize(gray_img, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "test_image = resized_img.reshape((1, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "34489551-60d9-485f-92eb-b5169bf7d5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "the number for the picture is :  [[5.8776024e-04 4.3954285e-12 3.0337956e-03 4.2990020e-01 1.0440487e-10\n",
      "  4.7964576e-01 5.6599438e-06 3.7058842e-07 7.8779176e-02 8.0473293e-03]]\n"
     ]
    }
   ],
   "source": [
    "res = network.predict(test_image)\n",
    "print(\"the number for the picture is : \", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "de7947d2-dde9-415a-90b1-c07168e472a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "the number for the picture is :  2\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "digit = test_images[1]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "test_images = test_images.reshape((10000, 28*28))\n",
    "res = network.predict(test_images)\n",
    "print(res)\n",
    "\n",
    "for i in range(res[1].shape[0]):\n",
    "    if (res[1][i] == 1):\n",
    "        print(\"the number for the picture is : \", i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068ae35-55e0-434b-8672-1dcf0a28c47e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
